---
content_type: page
title: Lecture Notes
uid: 99ddf4a2-3a6e-f9c4-342b-b24697590209
---

The following lecture notes were written for 6.441 by Professors Yury Polyanskiy of MIT and Yihong Wu of University of Illinois Urbana-Champaign. A complete copy of the notes are![This resource may not render correctly in a screen reader.](/images/inacessible.gif) [available for download (PDF - 7.6MB)]({{< baseurl >}}/resources/mit6_441s16_course_notes).

| CHAPTERS | SECTIONS |
| --- | --- |
| {{< td-colspan 2 >}}**Part I: Information Measures**{{< /td-colspan >}} ||
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 1: Information measures: Entropy and divergence (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_1) |  {{< br >}}{{< br >}} 1.1 Entropy {{< br >}}{{< br >}} 1.2 Divergence {{< br >}}{{< br >}} 1.3 Differential entropy {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 2: Information measures: Mutual information (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_2) |  {{< br >}}{{< br >}} 2.1 Divergence: Main inequality {{< br >}}{{< br >}} 2.2 Conditional divergence {{< br >}}{{< br >}} 2.3 Mutual information {{< br >}}{{< br >}} 2.4 Conditional mutual information and conditional independence {{< br >}}{{< br >}} 2.5 Strong data-processing inequalities {{< br >}}{{< br >}} 2.6 How to avoid measurability problems? {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 3: Sufficient statistic. Continuity of divergence and mutual information. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_3) |  {{< br >}}{{< br >}} 3.1 Sufficient statistics and data-processing {{< br >}}{{< br >}} 3.2 Geometric interpretation of mutual information {{< br >}}{{< br >}} 3.3 Variational characterizations of divergence: Donsker-Varadhan {{< br >}}{{< br >}} 3.4 Variational characterizations of divergence: Gelfand-Yaglom-Perez {{< br >}}{{< br >}} 3.5 Continuity of divergence. Dependence on sigma-algebra {{< br >}}{{< br >}} 3.6 Variational characterizations and continuity of mutual information {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 4: Extremization of mutual information: Capacity saddle point (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_4) |  {{< br >}}{{< br >}} 4.1 Convexity of information measures {{< br >}}{{< br >}} 4.2 Local behavior of divergence {{< br >}}{{< br >}} 4.3 Local behavior of divergence and Fisher information {{< br >}}{{< br >}} 4.4 Extremization of mutual information {{< br >}}{{< br >}} 4.5 Capacity = information radius {{< br >}}{{< br >}} 4.6 Existence of caod (general case) {{< br >}}{{< br >}} 4.7 Gaussian saddle point {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 5: Single-letterization. Probability of error. Entropy rate. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_5) |  {{< br >}}{{< br >}} 5.1 Extremization of mutual information for memoryless sources and channels {{< br >}}{{< br >}} 5.2 Gaussian capacity via orthogonal symmetry {{< br >}}{{< br >}} 5.3 Information measures and probability of error {{< br >}}{{< br >}} 5.4 Fano, LeCam and minimax risks {{< br >}}{{< br >}} 5.5 Entropy rate {{< br >}}{{< br >}} 5.6 Entropy and symbol (bit) error rate {{< br >}}{{< br >}} 5.7 Mutual information rate {{< br >}}{{< br >}} 5.8 Toeplitz matrices and Szego's theorem {{< br >}}{{< br >}}  |
| {{< td-colspan 2 >}}**Part II: Lossless Data Compression**{{< /td-colspan >}} ||
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 6: Variable-length Lossless Compression (PDF - 1.1MB)]({{< baseurl >}}/resources/mit6_441s16_chapter_6) |  {{< br >}}{{< br >}} 6.1 Variable-length, lossless, optimal compressor {{< br >}}{{< br >}} 6.2 Uniquely decodable codes, prefix codes and Huffman codes {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 7: Fixed-length (almost lossless) compression. Slepian-Wolf problem. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_7) |  {{< br >}}{{< br >}} 7.1 Fixed-length code, almost lossless {{< br >}}{{< br >}} 7.2 Linear Compression {{< br >}}{{< br >}} 7.3 Compression with Side Information at both compressor and decompressor {{< br >}}{{< br >}} 7.4 Slepian-Wolf (Compression with Side Information at Decompressor only) {{< br >}}{{< br >}} 7.5 Multi-terminal Slepian Wolf {{< br >}}{{< br >}} 7.6 Source-coding with a helper (Ahlswede-Korner-Wyner) {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 8: Compressing stationary ergodic sources (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_8) |  {{< br >}}{{< br >}} 8.1 Bits of ergodic theory {{< br >}}{{< br >}} 8.2 Proof of Shannon-McMillan {{< br >}}{{< br >}} 8.3 Proof of Birkhoff -Khintchine {{< br >}}{{< br >}} 8.4 Sinai's generator theorem {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 9: Universal compression (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_9) |  {{< br >}}{{< br >}} 9.1 Arithmetic coding {{< br >}}{{< br >}} 9.2 Combinatorial construction of Fitingof {{< br >}}{{< br >}} 9.3 Optimal compressors for a class of sources. Redundancy {{< br >}}{{< br >}} 9.4 Approximate minimax solution: Je\_reys prior {{< br >}}{{< br >}} 9.5 Sequential probability assignment: Krichevsky-Trofimov {{< br >}}{{< br >}} 9.6 Lempel-Ziv compressor {{< br >}}{{< br >}}  |
| {{< td-colspan 2 >}}**Part III: Binary Hypothesis Testing**{{< /td-colspan >}} ||
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 10: Binary hypothesis testing (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_10) |  {{< br >}}{{< br >}} 10.1 Binary Hypothesis Testing {{< br >}}{{< br >}} 10.2 Neyman-Pearson formulation {{< br >}}{{< br >}} 10.3 Likelihood ratio tests {{< br >}}{{< br >}} 10.4 Converse bounds on R(P, Q) {{< br >}}{{< br >}} 10.5 Achievability bounds on R(P,Q) {{< br >}}{{< br >}} 10.6 Asymptotics {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 11: Hypothesis testing asymptotics I (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_11) |  {{< br >}}{{< br >}} 11.1 Stein's regime {{< br >}}{{< br >}} 11.2 Chernoff regime {{< br >}}{{< br >}} 11.3 Basics of Large deviation theory {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 12: Information projection and Large deviation (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_12) |  {{< br >}}{{< br >}} 12.1 Large-deviation exponents {{< br >}}{{< br >}} 12.2 Information Projection {{< br >}}{{< br >}} 12.3 Interpretation of Information Projection {{< br >}}{{< br >}} 12.4 Generalization: Sanov's theorem {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 13: Hypothesis testing asymptotics II (PDF - 2.0MB)]({{< baseurl >}}/resources/mit6_441s16_chapter_13) |  {{< br >}}{{< br >}} 13.1 (E0,E1)-Tradeoff {{< br >}}{{< br >}} 13.2 Equivalent forms of Theorem 13.1 {{< br >}}{{< br >}} 13.3 Sequential Hypothesis Testing {{< br >}}{{< br >}}  |
| {{< td-colspan 2 >}}**Part IV: Channel Coding**{{< /td-colspan >}} ||
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 14: Channel coding (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_14) |  {{< br >}}{{< br >}} 14.1 Channel Coding {{< br >}}{{< br >}} 14.2 Basic Results {{< br >}}{{< br >}} 14.3 General (Weak) Converse Bounds {{< br >}}{{< br >}} 14.4 General achievability bounds: Preview {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 15: Channel coding: Achievability bounds (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_15) |  {{< br >}}{{< br >}} 15.1 Information density {{< br >}}{{< br >}} 15.2 Shannon's achievability bound {{< br >}}{{< br >}} 15.3 Dependence-testing bound {{< br >}}{{< br >}} 15.4 Feinstein's Lemma {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 16: Linear codes. Channel capacity. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_16) |  {{< br >}}{{< br >}} 16.1 Linear coding {{< br >}}{{< br >}} 16.2 Channels and channel capacity {{< br >}}{{< br >}} 16.3 Bounds on C\_e; Capacity of Stationary Memoryless Channels {{< br >}}{{< br >}} 16.4 Examples of DMC {{< br >}}{{< br >}} 16.5 Information Stability {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 17: Channels with input constraints. Gaussian channels. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_17) |  {{< br >}}{{< br >}} 17.1 Channel coding with input constraints {{< br >}}{{< br >}} 17.2 Capacity under input constraint C(P) ?= Ci(P) {{< br >}}{{< br >}} 17.3 Applications {{< br >}}{{< br >}} 17.4 Non-stationary AWGN {{< br >}}{{< br >}} 17.5 Stationary Additive Colored Gaussian noise channel {{< br >}}{{< br >}} 17.6 Additive White Gaussian Noise channel with Intersymbol Interference {{< br >}}{{< br >}} 17.7 Gaussian channels with amplitude constraints {{< br >}}{{< br >}} 17.8 Gaussian channels with fading {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 18: Lattice codes (by O. Ordentlich) (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_18) |  {{< br >}}{{< br >}} 18.1 Lattice Definitions {{< br >}}{{< br >}} 18.2 First Attempt at AWGN Capacity {{< br >}}{{< br >}} 18.3 Nested Lattice Codes/Voronoi Constellations {{< br >}}{{< br >}} 18.4 Dirty Paper Coding {{< br >}}{{< br >}} 18.5 Construction of Good Nested Lattice Pairs {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 19: Channel coding: Energy-per-bit, continuous-time channels (PDF - 1.1MB)]({{< baseurl >}}/resources/mit6_441s16_chapter_19) |  {{< br >}}{{< br >}} 19.1 Energy per bit {{< br >}}{{< br >}} 19.2 What is N0? {{< br >}}{{< br >}} 19.3 Capacity of the continuous-time band-limited AWGN channel {{< br >}}{{< br >}} 19.4 Capacity of the continuous-time band-unlimited AWGN channel {{< br >}}{{< br >}} 19.5 Capacity per unit cost {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 20: Advanced channel coding. Source-Channel separation. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_20) |  {{< br >}}{{< br >}} 20.1 Strong Converse {{< br >}}{{< br >}} 20.2 Stationary memoryless channel without strong converse {{< br >}}{{< br >}} 20.3 Channel Dispersion {{< br >}}{{< br >}} 20.4 Normalized Rate {{< br >}}{{< br >}} 20.5 Joint Source Channel Coding {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 21: Channel coding with feedback (PDF - 1.2MB)]({{< baseurl >}}/resources/mit6_441s16_chapter_21) |  {{< br >}}{{< br >}} 21.1 Feedback does not increase capacity for stationary memoryless channels {{< br >}}{{< br >}} 21.2 Alternative proof of Theorem 21.1 and Massey's directed information {{< br >}}{{< br >}} 21.3 When is feedback really useful? {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 22: Capacity-achieving codes via Forney concatenation (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_22) |  {{< br >}}{{< br >}} 22.1 Error exponents {{< br >}}{{< br >}} 22.2 Achieving polynomially small error probability {{< br >}}{{< br >}} 22.3 Concatenated codes {{< br >}}{{< br >}} 22.4 Achieving exponentially small error probability {{< br >}}{{< br >}}  |
| {{< td-colspan 2 >}}**Part V: Lossy Data Compression**Â {{< /td-colspan >}} ||
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 23: Rate-distortion theory (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_23) |  {{< br >}}{{< br >}} 23.1 Scalar quantization {{< br >}}{{< br >}} 23.2 Information-theoretic vector quantization {{< br >}}{{< br >}} 23.3 Converting excess distortion to average {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 24: Rate distortion: Achievability bounds (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_24) |  {{< br >}}{{< br >}} 24.1 Recap {{< br >}}{{< br >}} 24.2 Shannon's rate-distortion theorem {{< br >}}{{< br >}} 24.3 Covering lemma {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 25: Evaluating R(D). Lossy Source-Channel separation. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_25) |  {{< br >}}{{< br >}} 25.1 Evaluation of R(D) {{< br >}}{{< br >}} 25.2 Analog of saddle-point property in rate-distortion {{< br >}}{{< br >}} 25.3 Lossy joint source-channel coding {{< br >}}{{< br >}} 25.4 What is lacking in classical lossy compression? {{< br >}}{{< br >}}  |
| {{< td-colspan 2 >}}**Part VI: Advanced Topics**{{< /td-colspan >}} ||
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 26: Multiple-access channel (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_26) |  {{< br >}}{{< br >}} 26.1 Problem motivation and main results {{< br >}}{{< br >}} 26.2 MAC achievability bound {{< br >}}{{< br >}} 26.3 MAC capacity region proof {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 27: Examples of MACs. Maximal Pe and zero-error capacity. (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_27) |  {{< br >}}{{< br >}} 27.1 Recap {{< br >}}{{< br >}} 27.2 Orthogonal MAC {{< br >}}{{< br >}} 27.3 BSC MAC {{< br >}}{{< br >}} 27.4 Adder MAC {{< br >}}{{< br >}} 27.5 Multiplier MAC {{< br >}}{{< br >}} 27.6 Contraction MAC {{< br >}}{{< br >}} 27.7 Gaussian MAC {{< br >}}{{< br >}} 27.8 MAC Peculiarities {{< br >}}{{< br >}}  |
| ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Chapter 28: Random number generators (PDF)]({{< baseurl >}}/resources/mit6_441s16_chapter_28) |  {{< br >}}{{< br >}} 28.1 Setup {{< br >}}{{< br >}} 28.2 Converse {{< br >}}{{< br >}} 28.3 Elias' construction of RNG from lossless compressors {{< br >}}{{< br >}} 28.4 Peres' iterated von Neumann's scheme {{< br >}}{{< br >}} 28.5 Bernoulli factory {{< br >}}{{< br >}} 28.6 Related problems {{< br >}}{{< br >}}